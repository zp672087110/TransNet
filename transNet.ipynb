{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TransNet: Translation-Based Network Representation Learning for Social Relation Extraction\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will use TransNet(a NRL model) to learn a social network and predict the labels on the social relations. We present the task of Social Relation Extraction(SRE), which is designed to extract relations between social network vertices.\n",
    "\n",
    "In TransNet, we focus on the problem of incorporating rich relation infomation on edgs into NRL(Network Representation Learning). \n",
    "\n",
    "<a data-flickr-embed=\"true\"  href=\"https://www.flickr.com/photos/150924720@N04/35463916440/\" title=\"framework\"><img src=\"https://farm5.staticflickr.com/4266/35463916440_7444da7c8c.jpg\" width=\"500\" height=\"403\" alt=\"framework\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "As shown in above figure, TransNet consists of two critical components, translation part and edge representation part. In the following parts, we first show how to implement translation mechanism.  Then we introduce how to construct the edge representations. At last, we give the overall objective function of TransNet.\n",
    "\n",
    "### Read datset\n",
    "\n",
    "Before constructing TransNet, we should read dataset first. To simplify this notebook, we use `read_data_sets()` to read data. This function will return a objecct which can generate training batches and testing batches. We choose a sub-network of ArnetMiner. This network have 1000 nodes and about 400 kinds of labels on relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from input_data import read_data_sets\n",
    "\n",
    "aminer = read_data_sets()\n",
    "entity_total = aminer.entity_total\n",
    "tag_total = aminer.tag_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we set some parameters. We will explain them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "warm_up_epochs = 20\n",
    "epochs = 66\n",
    "batch_size = 100\n",
    "eval_batch_size = 2000\n",
    "display_step = 5\n",
    "\n",
    "gamma = 1 # margin\n",
    "alpha = 0.5\n",
    "l2_lambda = 0.001 # regulizer weight\n",
    "beta = 50.0\n",
    "keep_prob = 0.5 # drop out prob\n",
    "rep_size = 64 # representation size\n",
    "\n",
    "hits_k = [1,5,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Translation Mechnism\n",
    "\n",
    "Motivated by translation mechanisms in word representations [Mikolov *et al*., 2013] and knowledge representations [Bordes *et al*., 2013], we assume that the interactions between vertices in social networks can also be portrayed as translations in the representation space.\n",
    "\n",
    "Specially, for each edge $e = (u, v)$ and its corresponding label set $l$, the representation of vertex $v$ is expected to be close to the representation of vertex $u$ plus the representation of edge $e$. As each vertex plays two roles in TransNet: head vertex and tail vertex, we introduce two vectors $\\bf{v}$ abd $\\bf{v'}$ for each vertex $v$, corresponding to its heading representation and tail representation. After that, the translation mechanism among $u$, $v$ and $e$ can be formalized as\n",
    "$$u + l \\approx v'$$\n",
    "Note that, $\\bf{l}$ is the edge representation obtained from $l$, which will be introduced in the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "# pos_h, pos_t, neg_h, neg_t are vertices' ID\n",
    "pos_h = tf.placeholder(tf.int32, [None])\n",
    "pos_t = tf.placeholder(tf.int32, [None])\n",
    "pos_r = tf.placeholder(tf.float32, [None, tag_total])\n",
    "pos_br = tf.placeholder(tf.float32, [None, tag_total])\n",
    "\n",
    "neg_h = tf.placeholder(tf.int32, [None])\n",
    "neg_t = tf.placeholder(tf.int32, [None])\n",
    "neg_r = tf.placeholder(tf.float32, [None, tag_total])\n",
    "neg_br = tf.placeholder(tf.float32, [None, tag_total])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we use the ID of vertex to get the head representation and tail representation of the vertex. We put these embeddings in `node_lookup` at next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#embedding\n",
    "node_lookup = {\n",
    "    'int_embeddings': tf.Variable(tf.random_normal([entity_total, rep_size])),\n",
    "    'adv_embeddings': tf.Variable(tf.random_normal([entity_total, rep_size])),\n",
    "}\n",
    "\n",
    "def lookup(pos_head, pos_tail, neg_head, neg_tail, lookup):\n",
    "    pos_head_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['int_embeddings'], pos_head), 1)\n",
    "    pos_tail_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['adv_embeddings'], pos_tail), 1)\n",
    "    neg_head_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['int_embeddings'], neg_head), 1)\n",
    "    neg_tail_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['adv_embeddings'], neg_tail), 1)\n",
    "    return pos_head_e, pos_tail_e, neg_head_e, neg_tail_e\n",
    "\n",
    "# pos_h_e, pos_t_e, neg_h_e, neg_t_e are representations of vertices\n",
    "pos_h_e, pos_t_e, neg_h_e, neg_t_e = lookup(pos_h, pos_t,\n",
    "                                           neg_h, neg_t, node_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Edge Representation Construction\n",
    "\n",
    "As shown in above figure, we employ a deep autoencoder to construct the edge representations. The encoder part composes of several non-linear transformation layers to transform the label set into a low-dimensional representation space. Moreover, the reconstruction process of the decoder part makes the representation preserve all the label information.\n",
    "\n",
    "To simplified this model, we just set one hidden layer and use it as representation of label. There are input layer, hidden layer and reconstruction layer. At hidden layer, we choose tanh as our activiation function to make output between -1 and 1. At reconstruction layer, we choose sigmoid as activiation function to make output between 0 and 1 because original label vectors only have 0,1 value.\n",
    "\n",
    "We store the weights and biases of the autoencoder in `relation_weights` and `relation_biases`. In order to prevent overfitting, we also employ dropout to generate the dge representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# autoencoder\n",
    "relation_weights = {\n",
    "    'encoder_w': tf.Variable(tf.random_normal([tag_total, rep_size])),\n",
    "    'decoder_w': tf.Variable(tf.random_normal([rep_size, tag_total])),\n",
    "}\n",
    "relation_biases = {\n",
    "    'encoder_b': tf.Variable(tf.random_normal([rep_size])),\n",
    "    'decoder_b': tf.Variable(tf.random_normal([tag_total])),\n",
    "}\n",
    "\n",
    "def autoencoder(W,B,x):\n",
    "    rep = tf.nn.dropout(\n",
    "        tf.nn.tanh(tf.matmul(x, W['encoder_w'])+B['encoder_b']), keep_prob)\n",
    "    decode_x = tf.nn.sigmoid(\n",
    "        tf.matmul(rep, W['decoder_w'])+B['decoder_b'])\n",
    "    return rep, decode_x\n",
    "\n",
    "# xxx_rep refer to the value of hidden layer\n",
    "# xxx_dec refer to the value of reconstruction layer\n",
    "pos_r_rep, pos_r_dec = autoencoder(relation_weights, relation_biases, pos_r)\n",
    "neg_r_rep, neg_r_dec = autoencoder(relation_weights, relation_biases, neg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Overall Architecture\n",
    "\n",
    "### Reconstruction Loss:\n",
    "\n",
    "Autoencoder aims to minimize the distance between inputs and the reconstructed outputs. The reconstruction loss is shown as:\n",
    "\n",
    "$$L_{rec} = ||s - \\hat{s}||$$\n",
    "\n",
    "Here, we also adopt L1-norm to measure the reconstruction distance. However, due to the sparsity of the input vector, the number of zero elements in s is much larger than that of non-zero elements. That means the autoencoder will tend to reconstruct the zero elements rather than non-zero ones, which is incompatible with our purpose. Therefore, we set different weights to different elements, and re-defined the loss function as follows:\n",
    "\n",
    "$$L_{ae} = ||(s-\\hat{s})\\odot x||$$\n",
    "\n",
    "Where x is a weight vector and $\\odot$ means the Hadamard product. For $x=\\{x_i\\}_{i=1}^{|T|}, x_i=1$ when $s_i=0$ and $x_i=\\beta > 1$ otherwise. Here we set $\\beta = 50.0$.\n",
    "\n",
    "We can look back the code block of input, `pos_br` and `neg_br` are the $x$ mentioned here. To get a powerful autoencoder before training translation model, we design a warm-up training for autoencoder. In warm-up training, we use label vectors from dataset as training data.\n",
    "\n",
    "With the utilization of deep autoencoder, the edge representation not only remains the critical information of corresponding labels, but also has the ability of predicting the relation (labels) between two vertices.\n",
    "\n",
    "To prevent overfitting, we define an L2-norm regularizer as:\n",
    "\n",
    "$$ L_{reg} = \\sum_{i=1}^{K}(||W^{(i)}||^2_2+||b^{(i)}||^2_2)$$\n",
    "\n",
    "Note that K is the number of the autoencoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "# L2-norm regularizer\n",
    "relation_ae_l2_loss = tf.nn.l2_loss(relation_weights['encoder_w'])+\\\n",
    "                        tf.nn.l2_loss(relation_weights['decoder_w'])+\\\n",
    "                        tf.nn.l2_loss(relation_biases['encoder_b'])+\\\n",
    "                        tf.nn.l2_loss(relation_biases['decoder_b'])\n",
    "# L1-norm to measure the reconstruction distance(reconstruction loss)\n",
    "relation_loss = tf.reduce_sum(tf.abs(tf.multiply(pos_r_dec-pos_r, pos_br)))+\\\n",
    "                tf.reduce_sum(tf.abs(tf.multiply(neg_r_dec-neg_r, neg_br)))\n",
    "# warm-up reconstruction loss\n",
    "relation_pos_r_loss = tf.reduce_sum(tf.abs(tf.multiply(pos_r_dec-pos_r, pos_br))) +\\\n",
    "                        l2_lambda*relation_ae_l2_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Translation loss\n",
    "\n",
    "We employ a distance function $d(u+l, v')$ to estimate the degree of $(u,v,l)$ that matches. In practice, we simply adopt $L_1$-norm. With the above definitions, for each $(u,v,l)$ and its negative sample $(\\hat{u}, \\hat{v}, \\hat{l})$, the translation part of TransNet aims to minimize the hinge-loss as follows:\n",
    "$$L_{trans}=max(\\gamma + d(u+l,v')-d(\\hat{u}+\\hat{l}, \\hat{v}', 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# positive distance\n",
    "pos = tf.reduce_sum(tf.abs(pos_h_e + pos_r_rep - pos_t_e), 1, keep_dims = True)\n",
    "# negative distance\n",
    "neg = tf.reduce_sum(tf.abs(neg_h_e + neg_r_rep - neg_t_e), 1, keep_dims = True)\n",
    "trans_loss = tf.reduce_sum(tf.maximum(pos - neg + gamma, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Overall loss\n",
    "\n",
    "To preserve the translation mechanism among vertex and\n",
    "edge representations, as well as the reconstruction ability of\n",
    "edge representations, we combine the objectives and propose a unified NRL model TransNet. For each $(u,v,l)$ and its negative sample $(\\hat{u}, \\hat{v}, \\hat{l})$, TransNet jointly optimizes the objective as follows:\n",
    "\n",
    "$$L = L_{trans} + \\alpha|L_{ae}(l)+L_{ae}(\\hat{l})|+\\eta L_{reg}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = trans_loss+alpha*relation_loss+l2_lambda*relation_ae_l2_loss\n",
    "# warm-up optimizer\n",
    "relation_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(relation_pos_r_loss)\n",
    "# overall loss optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction and Evaluation\n",
    "\n",
    "With the learnt vertex representations and the edge autoencoder, TransNet is capable of predicting the labels on the edges.We can get the approximate edge representation through $l=v'-u$. Naturally, we decode the edge representation l with the decoder part to obtain the predicted label vector $\\hat{s}$. A large weight $\\hat{s_i}$ indicates $t_i$ is more possible in $l$.\n",
    "\n",
    "We employ *hits@k* and *MeanRank*[Borders *et al.*, 2013] as evaluation metrics. Here, *MeanRank* is the mean of prediced ranks of all annotated labels, while *hits@k* means the propotion of correct labels ranked in the top $k$. Note that, the above metrics\n",
    "will under-estimate the models that rank other correct labels in the same label set high. Hence, we can filter out these labels before ranking. But here we don't filter out these labels to simplify the notebook.\n",
    "\n",
    "Here we set $k=1,5,10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "relation_sum = tf.reduce_sum(pos_r)\n",
    "pos_r_minus = pos_t_e - pos_h_e\n",
    "# pos_r_minus_dec is the predicted label vector hat s\n",
    "pos_r_minus_dec = tf.nn.sigmoid(\n",
    "    tf.matmul(pos_r_minus, relation_weights['decoder_w'])+relation_biases['decoder_b'])\n",
    "hits = []\n",
    "for k in hits_k:\n",
    "    # find the indices of top k labels\n",
    "    topk_indices = tf.nn.top_k(pos_r_minus_dec, k=k).indices\n",
    "    # transform indices to 0,1 vectors whose size is total label size\n",
    "    pred = tf.reduce_sum(tf.one_hot(topk_indices, tag_total), 1)\n",
    "    # compare preds and original relation labels to get correct numbers\n",
    "    hits.append(tf.reduce_sum(tf.multiply(pred, pos_r)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Launch the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting warm-up relation training\n",
      "Warm-up relation epoch:  0 sum of loss 5207895.86523\n",
      "Warm-up relation epoch:  1 sum of loss 3475534.05664\n",
      "Warm-up relation epoch:  2 sum of loss 2433169.40234\n",
      "Warm-up relation epoch:  3 sum of loss 1828799.39893\n",
      "Warm-up relation epoch:  4 sum of loss 1482831.60449\n",
      "Warm-up relation epoch:  5 sum of loss 1267588.05176\n",
      "Warm-up relation epoch:  6 sum of loss 1096929.78125\n",
      "Warm-up relation epoch:  7 sum of loss 974408.318604\n",
      "Warm-up relation epoch:  8 sum of loss 876401.463135\n",
      "Warm-up relation epoch:  9 sum of loss 788701.8125\n",
      "Warm-up relation epoch:  10 sum of loss 727057.040039\n",
      "Warm-up relation epoch:  11 sum of loss 675831.755371\n",
      "Warm-up relation epoch:  12 sum of loss 632049.754517\n",
      "Warm-up relation epoch:  13 sum of loss 589350.636597\n",
      "Warm-up relation epoch:  14 sum of loss 555669.37439\n",
      "Warm-up relation epoch:  15 sum of loss 521739.079102\n",
      "Warm-up relation epoch:  16 sum of loss 490935.845703\n",
      "Warm-up relation epoch:  17 sum of loss 465631.240845\n",
      "Warm-up relation epoch:  18 sum of loss 440318.335327\n",
      "Warm-up relation epoch:  19 sum of loss 423015.900635\n",
      "Train TransNet epoch:  0 sum of loss 3190995.36133\n",
      "Evaluating...\n",
      "Hits@ 1: 0.0124153498871\n",
      "Hits@ 5: 0.044920993228\n",
      "Hits@10: 0.0774266365688\n",
      "Train TransNet epoch:  1 sum of loss 3048742.61914\n",
      "Train TransNet epoch:  2 sum of loss 2944794.40039\n",
      "Train TransNet epoch:  3 sum of loss 2837108.16504\n",
      "Train TransNet epoch:  4 sum of loss 2748413.53027\n",
      "Train TransNet epoch:  5 sum of loss 2657619.4082\n",
      "Evaluating...\n",
      "Hits@ 1: 0.0331828442438\n",
      "Hits@ 5: 0.115237020316\n",
      "Hits@10: 0.181828442438\n",
      "Train TransNet epoch:  6 sum of loss 2580383.38672\n",
      "Train TransNet epoch:  7 sum of loss 2492281.2998\n",
      "Train TransNet epoch:  8 sum of loss 2396357.88379\n",
      "Train TransNet epoch:  9 sum of loss 2314071.32031\n",
      "Train TransNet epoch:  10 sum of loss 2248396.37891\n",
      "Evaluating...\n",
      "Hits@ 1: 0.0686230248307\n",
      "Hits@ 5: 0.230699774266\n",
      "Hits@10: 0.342663656885\n",
      "Train TransNet epoch:  11 sum of loss 2184037.44531\n",
      "Train TransNet epoch:  12 sum of loss 2134148.92773\n",
      "Train TransNet epoch:  13 sum of loss 2073480.80469\n",
      "Train TransNet epoch:  14 sum of loss 2025958.29443\n",
      "Train TransNet epoch:  15 sum of loss 1966639.05859\n",
      "Evaluating...\n",
      "Hits@ 1: 0.102257336343\n",
      "Hits@ 5: 0.343566591422\n",
      "Hits@10: 0.485665914221\n",
      "Train TransNet epoch:  16 sum of loss 1925018.56689\n",
      "Train TransNet epoch:  17 sum of loss 1886266.27002\n",
      "Train TransNet epoch:  18 sum of loss 1853101.12793\n",
      "Train TransNet epoch:  19 sum of loss 1815825.55029\n",
      "Train TransNet epoch:  20 sum of loss 1774167.43945\n",
      "Evaluating...\n",
      "Hits@ 1: 0.132054176072\n",
      "Hits@ 5: 0.423589164786\n",
      "Hits@10: 0.590519187359\n",
      "Train TransNet epoch:  21 sum of loss 1742555.94678\n",
      "Train TransNet epoch:  22 sum of loss 1711513.02441\n",
      "Train TransNet epoch:  23 sum of loss 1680351.23828\n",
      "Train TransNet epoch:  24 sum of loss 1659787.29346\n",
      "Train TransNet epoch:  25 sum of loss 1635447.21875\n",
      "Evaluating...\n",
      "Hits@ 1: 0.152483069977\n",
      "Hits@ 5: 0.4769751693\n",
      "Hits@10: 0.664221218962\n",
      "Train TransNet epoch:  26 sum of loss 1604657.24121\n",
      "Train TransNet epoch:  27 sum of loss 1587607.61035\n",
      "Train TransNet epoch:  28 sum of loss 1573381.65283\n",
      "Train TransNet epoch:  29 sum of loss 1552478.97754\n",
      "Train TransNet epoch:  30 sum of loss 1531602.65137\n",
      "Evaluating...\n",
      "Hits@ 1: 0.165124153499\n",
      "Hits@ 5: 0.517494356659\n",
      "Hits@10: 0.714898419865\n",
      "Train TransNet epoch:  31 sum of loss 1514297.02441\n",
      "Train TransNet epoch:  32 sum of loss 1501274.90674\n",
      "Train TransNet epoch:  33 sum of loss 1489680.28076\n",
      "Train TransNet epoch:  34 sum of loss 1472289.93311\n",
      "Train TransNet epoch:  35 sum of loss 1457000.6875\n",
      "Evaluating...\n",
      "Hits@ 1: 0.172686230248\n",
      "Hits@ 5: 0.546275395034\n",
      "Hits@10: 0.748532731377\n",
      "Train TransNet epoch:  36 sum of loss 1455656.45654\n",
      "Train TransNet epoch:  37 sum of loss 1438063.35986\n",
      "Train TransNet epoch:  38 sum of loss 1427074.01611\n",
      "Train TransNet epoch:  39 sum of loss 1416316.97461\n",
      "Train TransNet epoch:  40 sum of loss 1404773.98682\n",
      "Evaluating...\n",
      "Hits@ 1: 0.180135440181\n",
      "Hits@ 5: 0.566930022573\n",
      "Hits@10: 0.777426636569\n",
      "Train TransNet epoch:  41 sum of loss 1400461.36279\n",
      "Train TransNet epoch:  42 sum of loss 1387465.36963\n",
      "Train TransNet epoch:  43 sum of loss 1379927.01953\n",
      "Train TransNet epoch:  44 sum of loss 1374917.01074\n",
      "Train TransNet epoch:  45 sum of loss 1363523.90039\n",
      "Evaluating...\n",
      "Hits@ 1: 0.184875846501\n",
      "Hits@ 5: 0.584988713318\n",
      "Hits@10: 0.799548532731\n",
      "Train TransNet epoch:  46 sum of loss 1361686.80713\n",
      "Train TransNet epoch:  47 sum of loss 1349811.34961\n",
      "Train TransNet epoch:  48 sum of loss 1341667.14404\n",
      "Train TransNet epoch:  49 sum of loss 1327308.20312\n",
      "Train TransNet epoch:  50 sum of loss 1330428.04004\n",
      "Evaluating...\n",
      "Hits@ 1: 0.18769751693\n",
      "Hits@ 5: 0.600225733634\n",
      "Hits@10: 0.818171557562\n",
      "Train TransNet epoch:  51 sum of loss 1320217.79541\n",
      "Train TransNet epoch:  52 sum of loss 1320635.96826\n",
      "Train TransNet epoch:  53 sum of loss 1320296.02246\n",
      "Train TransNet epoch:  54 sum of loss 1309711.20166\n",
      "Train TransNet epoch:  55 sum of loss 1301525.49951\n",
      "Evaluating...\n",
      "Hits@ 1: 0.190180586907\n",
      "Hits@ 5: 0.61151241535\n",
      "Hits@10: 0.831264108352\n",
      "Train TransNet epoch:  56 sum of loss 1298060.98877\n",
      "Train TransNet epoch:  57 sum of loss 1286648.9502\n",
      "Train TransNet epoch:  58 sum of loss 1285108.82129\n",
      "Train TransNet epoch:  59 sum of loss 1284760.0166\n",
      "Train TransNet epoch:  60 sum of loss 1278184.95508\n",
      "Evaluating...\n",
      "Hits@ 1: 0.192099322799\n",
      "Hits@ 5: 0.620880361174\n",
      "Hits@10: 0.84164785553\n",
      "Train TransNet epoch:  61 sum of loss 1274818.77393\n",
      "Train TransNet epoch:  62 sum of loss 1270655.5708\n",
      "Train TransNet epoch:  63 sum of loss 1262444.87402\n",
      "Train TransNet epoch:  64 sum of loss 1264618.60059\n",
      "Train TransNet epoch:  65 sum of loss 1247946.49902\n",
      "Evaluating...\n",
      "Hits@ 1: 0.193792325056\n",
      "Hits@ 5: 0.629571106095\n",
      "Hits@10: 0.849661399549\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "total_batch = int(aminer.train.num_examples / batch_size)\n",
    "test_total_batch = int(aminer.test.num_examples / eval_batch_size)\n",
    "\n",
    "# initialize relation\n",
    "print \"Starting warm-up relation training\"\n",
    "for epoch in range(warm_up_epochs):\n",
    "    # loop over all batches\n",
    "    sum_loss = 0.0\n",
    "    for i in range(total_batch):\n",
    "        vecs, bs = aminer.train.next_autoencoder_batch(batch_size, beta)\n",
    "        _, cur_loss = sess.run([relation_optimizer, relation_pos_r_loss],\n",
    "                              feed_dict={pos_r: vecs, pos_br: bs})\n",
    "        sum_loss += cur_loss\n",
    "    print 'Warm-up relation epoch: ', epoch, 'sum of loss', sum_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sum_loss = 0.0\n",
    "    for i in range(total_batch):\n",
    "        pos_h_batch, pos_t_batch, pos_r_batch, pos_b_batch,\\\n",
    "        neg_h_batch, neg_t_batch, neg_r_batch, neg_b_batch = aminer.train.next_batch(batch_size, beta)\n",
    "        _, cur_loss = sess.run([optimizer, loss],\n",
    "                               feed_dict={pos_h: pos_h_batch, pos_t: pos_t_batch,\n",
    "                                         pos_r: pos_r_batch, pos_br: pos_b_batch,\n",
    "                                         neg_h: neg_h_batch, neg_t: neg_t_batch,\n",
    "                                         neg_r: neg_r_batch, neg_br: neg_b_batch})\n",
    "        sum_loss += cur_loss\n",
    "    print 'Train TransNet epoch: ', epoch, 'sum of loss', sum_loss\n",
    "    if epoch % display_step == 0:\n",
    "        print 'Evaluating...'\n",
    "        hits_ = [0]*len(hits_k)\n",
    "        all_count = 0.0\n",
    "        for i in range(test_total_batch):\n",
    "            pos_h_batch, pos_t_batch, pos_r_batch = aminer.test.next_test_batch(eval_batch_size)\n",
    "            cur_hits, cur_sum = sess.run([hits, relation_sum],\n",
    "                                        feed_dict={pos_h: pos_h_batch,\n",
    "                                                  pos_t: pos_t_batch,\n",
    "                                                  pos_r: pos_r_batch})\n",
    "            hits_ = list(map(lambda x: x[0]+x[1], zip(hits_, cur_hits)))\n",
    "            all_count +=cur_sum\n",
    "        r = [hit/all_count for hit in hits_]\n",
    "        for j, hit in enumerate(hits_k):\n",
    "            print 'Hits@{:2d}: {}'.format(hit, r[j])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
